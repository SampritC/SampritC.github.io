<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>VEB Package | Samprit Chakraborty</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description"
        content="Computational aspects of a mean field approach to empirical Bayes estimation in high-dimensional linear regression" />
  <link rel="stylesheet" href="assets/css/style.css" />

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=IBM+Plex+Sans:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />

  <!-- Optional MathJax if you add formulas -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>
  <div class="site-shell">
    <!-- Shared header -->
    <header class="site-header">
      <div class="header-inner">
        <div class="header-text">
          <div class="site-name">Samprit Chakraborty</div>
          <div class="site-tagline">
            Variational Empirical Bayes · High-Dimensional Regression
          </div>
          <div class="site-affiliation">
            
          </div>
          <div class="header-links">
            <a href="mailto:sampritcstat@gmail.com">Email</a>
            <a href="https://github.com/SampritC" target="_blank" rel="noopener">GitHub</a>
            <a href="index.html">Home</a> 
          </div>
        </div>
        <div class="header-right">
          <div class="photo-frame">
            <img src="assets/img/samprit.jpeg" alt="Samprit Chakraborty" />
          </div>
          <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
            ◐ Theme
          </button>
        </div>
      </div>
    </header>

    <!-- Two-column layout -->
    <div class="page-layout">
      <!-- Left contents -->
      <aside class="side-menu">
        <div class="side-menu-title">Contents</div>
        <nav>
          <a href="index.html#projects">⟵ Back to Projects</a>
           <a href="#Introduction">Introduction</a>
          <a href="#Problem setup">Problem setup</a>
          <a href="#prior-evolution">Prior evolution</a>
          <a href="#Simulations">Posterior Mean vs True Coefficients</a>
          <a href="#Comparison with Lasso Relative to True Coefficients">Comparison with Lasso Relative to True Coefficients</a>
          <a href="#Comparison with OLS and LASSO">Comparison with OLS and LASSO</a>
          <a href="#convergence">Convergence diagnostics</a>
          <a href="#theorem">Simulation to Theory</a>
        </nav>
      </aside>

      <!-- Right content -->
      <main class="main-content">
        <!-- Overview -->
        <section id="Introduction" class="section">
          <h2>Introduction</h2>
          <p>
  On this page, I present the simulation results and theoretical findings from my working paper
  <em>Solving Empirical Bayes Estimation in High-Dimensional Linear Regression Using a Mean-Field Approach</em>,
  which is part of my Master's thesis supervised by
  <a href="https://sites.stat.columbia.edu/bodhi/Bodhi/Welcome.html" target="_blank" rel="noopener">Prof. Bodhisattva Sen</a>.
  This work focuses on the computational aspects of
  <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">
    A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression
  </a>.
  We develop an algorithm for this problem and compare its performance with LASSO, OLS, MCMC, and we also compare our algorithm with existing CAVI method.
  We also establish several theoretical results for our algorithm.
</p>
</section>

        <section id="Problem setup" class="section">
          <h2>Problem setup</h2>
          <p>
    We study an empirical Bayes approach to high-dimensional Gaussian linear regression. 
    The data follow
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \varepsilon,\quad 
    \varepsilon \sim \mathcal{N}\!\left(0, \sigma^2 \mathbf{I}_n\right),
    \]
  </p>

  <p>
    where \(\mathbf{y} \in \mathbb{R}^n\) is the response vector, 
    \(\mathbf{X} \in \mathbb{R}^{n \times p}\) is the design matrix with columns 
    \(\mathbf{x}_1, \dots, \mathbf{x}_p\), \(\boldsymbol{\beta} = (\beta_1,\dots,\beta_p)^\top\) 
    are the unknown regression coefficients, and \(\sigma^2 > 0\) is treated as known.
  </p>

  <p>
    In an empirical Bayes formulation, we place an unknown prior \(\mu\) on each coefficient and assume
    \(\beta_j \overset{\text{iid}}{\sim} \mu\) for \(j = 1,\dots,p\). 
    We approximate \(\mu\) by a discrete distribution supported on a fixed grid 
    \(\mathcal{A} = \{a_1 < \dots < a_k\} \subset \mathbb{R}\), for example a fine grid in \([-1.5, 1.5]\) (suppose support of the prior is \([-1, 1]\), so keep lower and upper value of these grid points a little relaxed):
  </p>

  <p style="text-align:center;">
    \[
    \mu(\mathrm{d}t) \approx \sum_{\ell=1}^k \pi_\ell \,\delta_{a_\ell}(\mathrm{d}t),
    \quad 
    \boldsymbol{\pi} = (\pi_1,\dots,\pi_k)^\top \in \Delta^{k-1},
    \]
  </p>

  <p>
    where \(\Delta^{k-1} = \{\boldsymbol{\pi}: \pi_\ell \ge 0,\ \sum_{\ell=1}^k \pi_\ell = 1\}\) is the simplex 
    and \(\delta_{a_\ell}\) is a point mass at \(a_\ell\). The empirical Bayes goal is to learn 
    \(\boldsymbol{\pi}\) from the data.
  </p>

  <p>
    Instead of maximizing the intractable marginal likelihood
    \(p(\mathbf{y} \mid \boldsymbol{\pi})\), we use a mean-field variational approximation, where we approximate the posterior with a factorized distribution from the mean-field family
  </p>

  <p style="text-align:center;">
    \[
    q(\boldsymbol{\beta}) = \prod_{j=1}^p q_j(\beta_j),
    \quad
    q_j(\beta_j) = \sum_{\ell=1}^k r_{j\ell}\,\delta_{a_\ell}(\beta_j),
    \quad 
    \mathbf{r}_j = (r_{j1}, \dots, r_{jk})^\top \in \Delta^{k-1},
    \]
  </p>

  <p>
    and jointly optimize the mixing weights \(\boldsymbol{\pi}\) and the local weights 
    \(\{\mathbf{r}_j\}_{j=1}^p\) by maximizing the corresponding evidence lower bound (ELBO).
    From the optimal variational distribution, we form the coordinate-wise posterior mean
  </p>
      <p style="text-align:center;">
    \[
    \boldsymbol{u}
    \;=\;
    \mathbb{E}_q[\boldsymbol{\beta}]
    =
    (u_1,\dots,u_p)^\top,
    \qquad
    u_j
    =
    \sum_{\ell=1}^k a_\ell\, r_{j\ell},
    \]
  </p>

  <p>
    which serves as the empirical Bayes estimator of \(\boldsymbol{\beta}\) produced by our algorithm.
    The simulation results below compare this estimator against OLS, Lasso, MCMC-based benchmarks,
    and existing CAVI-type methods.
  </p>
</section>
      

        <!-- Figures: stacked vertically -->
        <section id="Simulations" class="section">
  <h2>Simulation Setup</h2>

  <p>
    Unless stated otherwise, the simulation results below are based on the following baseline design.
  </p>

  <p style="text-align:center;">
    \[
    n = 800,\qquad p = 50.
    \]
  </p>

  <p>
    The design matrix \(\mathbf{X} \in \mathbb{R}^{n \times p}\) has entries
    \(\mathbf{X}_{ij} \sim \mathcal{N}(0, 1/n)\), i.e.
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{X} = \frac{1}{\sqrt{n}} \mathbf{Z},
    \quad \mathbf{Z}_{ij} \sim \mathcal{N}(0,1),
    \]
  </p>

  <p>
    so that \(\|\mathbf{x}_j\|^2 = O(1)\) for each column, in line with Assumption&nbsp;1 of
    <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">Mukherjee et al.</a>
  </p>

  <p>
    The true coefficients \(\boldsymbol{\beta}^\star\) are three-point mass:
  </p>

  <p style="text-align:center;">
    \[
    \beta_j^\star \in \{-1, 0, 1\},\quad
    \mathbb{P}(\beta_j^\star = -1) = 0.3,\ 
    \mathbb{P}(\beta_j^\star = 0) = 0.4,\ 
    \mathbb{P}(\beta_j^\star = 1) = 0.3,
    \]
  </p>

  <p>
    and the responses are generated as
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta}^\star + \varepsilon,
    \qquad
    \varepsilon \sim \mathcal{N}(0, \sigma^2 \mathbf{I}_n),
    \]
  </p>

  <p>
    where \(\sigma^2\) is chosen to yield a non-trivial signal-to-noise regime; all methods are compared under this common design.
  </p>

</section>

      <section id="prior-evolution" class="section">
  <h2>Prior Evolution</h2>
  <p>
    The clip below shows how the estimated prior mass on the grid \(\mathcal{A}\) evolves over
    iterations of our algorithm. The mass gradually concentrates near the signal
    values, illustrating how the empirical Bayes procedure adapts to the underlying sparsity pattern.
  </p>

  <video class="veb-video" controls preload="metadata">
    <source src="assets/video/prior_evolution_strong_changes.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</section>
      <section id="Simulations" class="section">
        <h2>Posterior Mean vs True Coefficients</h2>
        <div class="project-card">
  <div class="project-name">Figure 1: Posterior Mean vs True Coefficients</div>
  <img
    src="assets/img/veb_posterior_mean_vs_true_beta.jpeg"
    alt="Scatter of posterior mean estimates versus true beta coefficients with bubble size proportional to posterior variance."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
    Posterior mean estimates from the VEB procedure track the true coefficient values closely;
    bubble size encodes the corresponding posterior variance.
  </div>
</div>
</section>
        <section id="Comparison with Lasso Relative to True Coefficients" class="section">
        <h2>Comparison with Lasso Relative to True Coefficients</h2>
        <div class="project-card">
  <div class="project-name">
    Figure 2: Comparison with Lasso
  </div>
  <img
    src="assets/img/veb_lasso_vs_veb_truebeta.jpeg"
    alt="Side-by-side comparison: Lasso estimates versus true coefficients (left) and variational posterior means versus true coefficients (right)."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
  Left: Lasso estimates versus true coefficients, showing shrinkage toward zero and noticeable dispersion around the diagonal.
  Right: VEB posterior means align closely with the diagonal at \(-1, 0, 1\), indicating better recovery of the true signal.
</div>
</div>
</section>
    <section id="Comparison with OLS and LASSO" class="section">
       <h2>Comparison with OLS and LASSO</h2>
        <div class="project-card">
  <div class="project-name">
    Figure 3: VEB Compared with OLS and Lasso
  </div>
  <img
    src="assets/img/veb_ols_lasso_comparison.jpeg"
    alt="Vertical line plots comparing VEB estimates to OLS (left) and Lasso (right), with true coefficients marked."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
    Left: OLS versus VEB. Right: Lasso versus VEB. In both panels, vertical segments connect method-specific
    estimates to the same underlying coefficient, with red points marking the true values and orange points the
    VEB posterior means. The VEB estimates cluster tightly around the truth, while OLS and Lasso display larger
    dispersion and shrinkage effects.
  </div>
</div>
</section>
      <section id="convergence" class="section">
  <h2>Convergence Diagnostics</h2>
  <p>
    To illustrate stability of the variational procedure, we track the convergence of the estimated prior
    and the posterior mean across iterations of our algorithm.
  </p>

  <div class="video-row">
    <div class="video-col">
      <h3>W<sub>1</sub> Convergence of the Prior</h3>
      <p class="section-intro">
       Wasserstein-1 distance between the prior estimates at iterations \(t-1\) and \(t\).
        The decay toward zero shows stabilization of the learned prior on \(\mathcal{A}\).
      </p>
      <video class="veb-video" controls preload="metadata">
        <source src="assets/video/w1_convergence_700iter.mp4" type="video/mp4"/>
      </video>
    </div>
    <div class="video-col">
      <h3>L<sub>2</sub> Change in Posterior Mean</h3>
      <p class="section-intro">
        \(\big\|\hat{\boldsymbol{u}}^{(t)} - \hat{\boldsymbol{u}}^{(t-1)}\big\|_2\), the L<sub>2</sub>-norm of the
        difference between successive posterior mean iterates. Its decay toward zero reflects numerical
        convergence of the VEB posterior mean estimator.
      </p>
      <video class="veb-video" controls preload="metadata">
        <source src="assets/video/posterior_convergence_700iter.mp4" type="video/mp4" />
      </video>
    </div>
  </div>
</section>

      </main>
    </div>
  </div>
</section>
                <section id="theorem" class="section">
  <h2>Simulation to Theory</h2>

  <p>
    The two convergence videos above are not just a numerical curiosity: in the regime considered
    here, the <span style="white-space:nowrap;">L<sub>2</sub> change in the posterior mean</span> and
    the <span style="white-space:nowrap;">W<sub>1</sub> change in the prior</span> really do decay at
    essentially the <em>same geometric rate</em>. The theorem below formalises exactly what the
    animations are hinting at.
  </p>

  <p>
    In these simulations the algorithm runs in β–space on a fixed 1D grid
    \(\{a_\ell\}_{\ell=1}^k \subset [-A,A]\) (here \(A = 1.5\)). At outer iteration
    \(t = 0,1,2,\dots\), we track:
  </p>

  <ul>
    <li>the posterior means of the regression coefficients \(u^{(t)} \in \mathbb{R}^p\);</li>
    <li>the learned prior weights \(\pi^{(t)} \in \Delta^{k-1}\) on the grid;</li>
    <li>
      a two-layer update:
      <ul>
        <li><strong>EM step.</strong> Update \(\pi^{(t)} \mapsto \pi^{(t+1)}\) by averaging
          responsibilities across coordinates (standard EM for the mixing weights).</li>
        <li><strong>Inner optimisation step (BFGS).</strong> With \(\pi^{(t+1)}\) held fixed, refine
          the site-specific variational parameters by running a short BFGS loop on the inner
          variational objective, starting from the previous iterate. Near the limiting fixed point
          this inner objective is locally strongly convex in those parameters, so a few BFGS steps
          are enough to drive the inner gradient down to a small level. Informally: in this local
          regime, the inner solve is accurate enough that, to first order, the algorithm behaves
          as if we had updated \(\pi\) and then plugged in the exact optimal sites for that prior.
        </li>
      </ul>
      Unlike closed-form CAVI, this is a genuinely <em>two-layer EM + BFGS</em> scheme: we use a
      numerical inner solver rather than a closed-form site update, and this turns out to matter
      for the convergence behaviour below.
    </li>
  </ul>

  <p>
    To measure convergence, we monitor:
  </p>

  <ul>
    <li>
      The L<sub>2</sub> change in the posterior mean:
      \[
        \Delta_u^{(t)} := \big\| u^{(t)} - u^{(t-1)} \big\|_2.
      \]
    </li>
    <li>
      The W<sub>1</sub> change in the prior:
      \[
        \Delta_\pi^{(t)} := W_1\big(\pi^{(t)}, \pi^{(t-1)}\big),
      \]
      computed on the same fixed grid \(\{a_\ell\}\).
    </li>
  </ul>

  <p>
    The off-diagonal coupling induced by the design is captured by
    \[
      M \;:=\; \frac{X^\top X}{\sigma^2}
      - \mathrm{diag}\Big(\frac{(X^\top X)_{11}}{\sigma^2},\dots,
                           \frac{(X^\top X)_{pp}}{\sigma^2}\Big).
    \]
    In our simulation setup (Gaussian design with \(X = X^{\text{raw}}/\sqrt{n}\),
    \(X^{\text{raw}}_{ij} \sim N(0,1)\)), one can show
    \(\|M\|_{\mathrm{op}} = O_{\mathbb{P}}(\sqrt{p/n})\); combined with the
    high-dimensional regime \(p = o(n)\) assumed in
    <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">Mukherjee et al.</a>,
    this implies \(\|M\|_{\mathrm{op}} \to 0\), which is a key ingredient in the theoretical analysis.
    We only state the resulting theorem here and omit the proof on this page.
  </p>

  <h4>Theorem (Common-ratio convergence of the two diagnostics for this EM + BFGS scheme)</h4>

  <p>
    The phenomenon observed in the videos above is specific to the <em>two-layer</em> algorithm used
    here: an EM-style update of the prior \(\pi\), followed by a local BFGS refinement of the site
    parameters. It is <strong>not</strong> a generic property of all variational updates (for
    example, a closed-form batch CAVI step need not exhibit this behaviour).
  </p>

  <p>Assume:</p>

  <ol>
    <li><strong>Weak regression coupling.</strong>
      The off-diagonal interaction matrix \(M\) satisfies
      \(\|M\|_{\mathrm{op}} = O_{\mathbb{P}}(\sqrt{p/n})\), so under \(p = o(n)\) we have
      \(\|M\|_{\mathrm{op}} \to 0\).
    </li>

    <li><strong>Non-degenerate limiting prior.</strong>
      The algorithm converges to a fixed point \((u^\star,\pi^\star)\) where the limiting prior
      \(\pi^\star\) places positive mass on at least two grid points (the support of \(\pi^\star\)
      is not a single atom).
    </li>

    <li><strong>Two-layer EM + BFGS update (inner-solver specific).</strong>
      Each outer iteration \(t \mapsto t+1\) consists of:
      <ul>
        <li>an EM-style update \(\pi^{(t)} \mapsto \pi^{(t+1)}\) obtained by averaging the current
            responsibilities;</li>
        <li>a short BFGS run on the inner variational objective with \(\pi^{(t+1)}\) held fixed,
            starting from the previous site parameters and stopped once the inner gradient is
            numerically small. In a neighbourhood of the fixed point the inner objective is locally
            strongly convex in the site parameters, and BFGS tracks the local optimum with
            <em>first-order accuracy</em>.</li>
      </ul>
    </li>
  </ol>

  <p>
    Under these conditions there exists a constant \(\lambda \in (0,1)\) such that
    \[
      \lim_{t\to\infty}
      \frac{\Delta_u^{(t-1)}}{\Delta_u^{(t)}} \;=\;
      \lim_{t\to\infty}
      \frac{\Delta_\pi^{(t-1)}}{\Delta_\pi^{(t)}} \;=\; \frac{1}{\lambda}.
    \]
    In particular, for large \(t\),
    \[
      \Delta_u^{(t)} \approx c_u\,\lambda^{t}, \qquad
      \Delta_\pi^{(t)} \approx c_\pi\,\lambda^{t}
    \]
    for some constants \(c_u,c_\pi>0\). Up to a vertical rescaling, the two diagnostics therefore
    have the same geometric decay: this is exactly what the convergence videos are illustrating.
  </p>

  <p>
    <i>Remark.</i> The use of BFGS in the inner step is not just a numerical convenience: it is
    precisely what makes the inner update behave, locally, like the exact optimiser for the current
    prior. This “first-order exactness” is what drives the common-ratio limit for the two
    diagnostics. A generic closed-form CAVI update for the sites does not necessarily share this
    property, and may not produce the same matching behaviour.
  </p>

  <p>
    The constant \(\lambda\) is the dominant eigenvalue of the local linearisation of the full
    EM + BFGS map around the fixed point \((u^\star,\pi^\star)\). In other words, \(\lambda\) is the
    effective contraction factor of one outer iteration. Because the inner step is solved (locally)
    to first order, both diagnostics inherit this same rate, which explains why the
    <span style="white-space:nowrap;">L<sub>2</sub> change in posterior mean</span> and the
    <span style="white-space:nowrap;">W<sub>1</sub> change in the prior</span> have almost identical
    “shape” on a log scale.
  </p>

</section>

  <footer class="footer">
    © <span id="year"></span> Samprit Chakraborty · VEB 
  </footer>

  <script src="assets/js/main.js"></script>
</body>
</html>
