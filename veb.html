<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>VEB Package | Samprit Chakraborty</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description"
        content="Computational aspects of a mean field approach to empirical Bayes estimation in high-dimensional linear regression" />
  <link rel="stylesheet" href="assets/css/style.css" />

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=IBM+Plex+Sans:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />

  <!-- Optional MathJax if you add formulas -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>
  <div class="site-shell">
    <!-- Shared header -->
    <header class="site-header">
      <div class="header-inner">
        <div class="header-text">
          <div class="site-name">Samprit Chakraborty</div>
          <div class="site-tagline">
            Variational Empirical Bayes · High-Dimensional Regression
          </div>
          <div class="site-affiliation">
            
          </div>
          <div class="header-links">
            <a href="mailto:sampritcstat@gmail.com">Email</a>
            <a href="https://github.com/SampritC" target="_blank" rel="noopener">GitHub</a>
            <a href="index.html">Home</a> 
          </div>
        </div>
        <div class="header-right">
          <div class="photo-frame">
            <img src="assets/img/samprit.jpeg" alt="Samprit Chakraborty" />
          </div>
          <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
            ◐ Theme
          </button>
        </div>
      </div>
    </header>

    <!-- Two-column layout -->
    <div class="page-layout">
      <!-- Left contents -->
      <aside class="side-menu">
        <div class="side-menu-title">Contents</div>
        <nav>
          <a href="index.html#projects">⟵ Back to Projects</a>
           <a href="#Introduction">Introduction</a>
          <a href="#Problem setup">Problem setup</a>
          <a href="#prior-evolution">Prior evolution</a>
          <a href="#Simulations">Simulation figures</a>
        </nav>
      </aside>

      <!-- Right content -->
      <main class="main-content">
        <!-- Overview -->
        <section id="Introduction" class="section">
          <h2>Introduction</h2>
          <p>
  On this page, I present the simulation results and theoretical findings from my working paper
  <em>"Solving Empirical Bayes Estimation in High-Dimensional Linear Regression Using a Mean-Field Approach"</em>,
  which is part of my Master's thesis supervised by
  <a href="https://sites.stat.columbia.edu/bodhi/Bodhi/Welcome.html" target="_blank" rel="noopener">Prof. Bodhisattva Sen</a>.
  This work focuses on the computational aspects of
  <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">
    "A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression"
  </a>.
  We develop an algorithm for this problem and compare its performance with LASSO, OLS, MCMC, and we also compare our algorithm with existing CAVI method.
  We also establish several theoretical results for our algorithm.
</p>
</section>

        <section id="Problem setup" class="section">
          <h2>Problem setup</h2>
          <p>
    We study an empirical Bayes approach to high-dimensional Gaussian linear regression. 
    The data follow
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \varepsilon,\quad 
    \varepsilon \sim \mathcal{N}\!\left(0, \sigma^2 \mathbf{I}_n\right),
    \]
  </p>

  <p>
    where \(\mathbf{y} \in \mathbb{R}^n\) is the response vector, 
    \(\mathbf{X} \in \mathbb{R}^{n \times p}\) is the design matrix with columns 
    \(\mathbf{x}_1, \dots, \mathbf{x}_p\), \(\boldsymbol{\beta} = (\beta_1,\dots,\beta_p)^\top\) 
    are the unknown regression coefficients, and \(\sigma^2 > 0\) is treated as known.
  </p>

  <p>
    In an empirical Bayes formulation, we place an unknown prior \(\mu\) on each coefficient and assume
    \(\beta_j \overset{\text{iid}}{\sim} \mu\) for \(j = 1,\dots,p\). 
    We approximate \(\mu\) by a discrete distribution supported on a fixed grid 
    \(\mathcal{A} = \{a_1 < \dots < a_k\} \subset \mathbb{R}\), for example a fine grid in \([-1.5, 1.5]\) (suppose support of the prior is \([-1, 1]\), so keep lower and upper value of these grid points a little relaxed):
  </p>

  <p style="text-align:center;">
    \[
    \mu(\mathrm{d}t) \approx \sum_{\ell=1}^k \pi_\ell \,\delta_{a_\ell}(\mathrm{d}t),
    \quad 
    \boldsymbol{\pi} = (\pi_1,\dots,\pi_k)^\top \in \Delta^{k-1},
    \]
  </p>

  <p>
    where \(\Delta^{k-1} = \{\boldsymbol{\pi}: \pi_\ell \ge 0,\ \sum_{\ell=1}^k \pi_\ell = 1\}\) is the simplex 
    and \(\delta_{a_\ell}\) is a point mass at \(a_\ell\). The empirical Bayes goal is to learn 
    \(\boldsymbol{\pi}\) from the data.
  </p>

  <p>
    Instead of maximizing the intractable marginal likelihood
    \(p(\mathbf{y} \mid \boldsymbol{\pi})\), we use a mean-field variational approximation, where we approximate the posterior with a factorized distribution from the mean-field family
  </p>

  <p style="text-align:center;">
    \[
    q(\boldsymbol{\beta}) = \prod_{j=1}^p q_j(\beta_j),
    \quad
    q_j(\beta_j) = \sum_{\ell=1}^k r_{j\ell}\,\delta_{a_\ell}(\beta_j),
    \quad 
    \mathbf{r}_j = (r_{j1}, \dots, r_{jk})^\top \in \Delta^{k-1},
    \]
  </p>

  <p>
    and jointly optimize the mixing weights \(\boldsymbol{\pi}\) and the local weights 
    \(\{\mathbf{r}_j\}_{j=1}^p\) by maximizing the corresponding evidence lower bound (ELBO).
    From the optimal variational distribution, we form the coordinate-wise posterior mean
  </p>
      <p style="text-align:center;">
    \[
    \boldsymbol{u}
    \;=\;
    \mathbb{E}_q[\boldsymbol{\beta}]
    =
    (u_1,\dots,u_p)^\top,
    \qquad
    u_j
    =
    \sum_{\ell=1}^k a_\ell\, r_{j\ell},
    \]
  </p>

  <p>
    which serves as the empirical Bayes estimator of \(\boldsymbol{\beta}\) produced by our algorithm.
    The simulation results below compare this estimator against OLS, Lasso, MCMC-based benchmarks,
    and existing CAVI-type methods.
  </p>
</section>
      

        <!-- Figures: stacked vertically -->
        <section id="Simulations" class="section">
  <h2>Simulation Setup</h2>

  <p>
    Unless stated otherwise, the simulation results below are based on the following baseline design.
  </p>

  <p style="text-align:center;">
    \[
    n = 800,\qquad p = 50.
    \]
  </p>

  <p>
    The design matrix \(\mathbf{X} \in \mathbb{R}^{n \times p}\) has entries
    \(\mathbf{X}_{ij} \sim \mathcal{N}(0, 1/n)\), i.e.
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{X} = \frac{1}{\sqrt{n}} \mathbf{Z},
    \quad \mathbf{Z}_{ij} \sim \mathcal{N}(0,1),
    \]
  </p>

  <p>
    so that \(\|\mathbf{x}_j\|^2 = O(1)\) for each column, in line with Assumption&nbsp;1 of
    <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">Mukherjee et al.</a>
  </p>

  <p>
    The true coefficients \(\boldsymbol{\beta}^\star\) are three-point sparse:
  </p>

  <p style="text-align:center;">
    \[
    \beta_j^\star \in \{-1, 0, 1\},\quad
    \mathbb{P}(\beta_j^\star = -1) = 0.3,\ 
    \mathbb{P}(\beta_j^\star = 0) = 0.4,\ 
    \mathbb{P}(\beta_j^\star = 1) = 0.3,
    \]
  </p>

  <p>
    and the responses are generated as
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta}^\star + \varepsilon,
    \qquad
    \varepsilon \sim \mathcal{N}(0, \sigma^2 \mathbf{I}_n),
    \]
  </p>

  <p>
    where \(\sigma^2\) is chosen to yield a non-trivial signal-to-noise regime; all methods are compared under this common design.
  </p>

</section>

      <section id="prior-evolution" class="section">
  <h2>Prior Evolution</h2>
  <p>
    The clip below shows how the estimated prior mass on the grid \(\mathcal{A}\) evolves over
    iterations of our algorithm. The mass gradually concentrates near the signal
    values, illustrating how the empirical Bayes procedure adapts to the underlying sparsity pattern.
  </p>

  <video class="veb-video" controls preload="metadata">
    <source src="assets/video/prior_evolution_strong_changes.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</section>
      <section id="Simulations" class="section">
        <h2>Posterior Mean vs True Coefficients</h2>
        <div class="project-card">
  <div class="project-name">Figure 1: Posterior Mean vs True Coefficients</div>
  <img
    src="assets/img/veb_posterior_mean_vs_true_beta.jpeg"
    alt="Scatter of posterior mean estimates versus true beta coefficients with bubble size proportional to posterior variance."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
    Posterior mean estimates from the VEB procedure track the true coefficient values closely;
    bubble size encodes the corresponding posterior variance.
  </div>
</div>
        <h2>Comparison with Lasso Relative to True Coefficients</h2>
        <div class="project-card">
  <div class="project-name">
    Figure 2: Comparison with Lasso
  </div>
  <img
    src="assets/img/veb_lasso_vs_veb_truebeta.jpeg"
    alt="Side-by-side comparison: Lasso estimates versus true coefficients (left) and variational posterior means versus true coefficients (right)."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
  Left: Lasso estimates versus true coefficients, showing shrinkage toward zero and noticeable dispersion around the diagonal.
  Right: VEB posterior means align closely with the diagonal at \(-1, 0, 1\), indicating better recovery of the true signal.
</div>
</div>
       <h2>Comparison with OLS and LASSO</h2>
        <div class="project-card">
  <div class="project-name">
    Figure 3: VEB Compared with OLS and Lasso
  </div>
  <img
    src="assets/img/veb_ols_lasso_comparison.jpeg"
    alt="Vertical line plots comparing VEB estimates to OLS (left) and Lasso (right), with true coefficients marked."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
    Left: OLS versus VEB. Right: Lasso versus VEB. In both panels, vertical segments connect method-specific
    estimates to the same underlying coefficient, with red points marking the true values and orange points the
    VEB posterior means. The VEB estimates cluster tightly around the truth, while OLS and Lasso display larger
    dispersion and shrinkage effects.
  </div>
</div>

</section>
      </main>
    </div>
  </div>

  <footer class="footer">
    © <span id="year"></span> Samprit Chakraborty · VEB Simulations
  </footer>

  <script src="assets/js/main.js"></script>
</body>
</html>
