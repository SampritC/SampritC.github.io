<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>VEB Package | Samprit Chakraborty</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description"
        content="Computational aspects of a mean field approach to empirical Bayes estimation in high-dimensional linear regression" />
  <link rel="stylesheet" href="assets/css/style.css" />

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=IBM+Plex+Sans:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />

  <!-- Optional MathJax if you add formulas -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>

<body>
  <div class="site-shell">
    <!-- Shared header -->
    <header class="site-header">
      <div class="header-inner">
        <div class="header-text">
          <div class="site-name">Samprit Chakraborty</div>
          <div class="site-tagline">
            Variational Empirical Bayes · High-Dimensional Regression
          </div>
          <div class="site-affiliation">
            
          </div>
          <div class="header-links">
            <a href="mailto:samprit.zidan@gmail.com">Email</a>
            <a href="https://github.com/SampritC" target="_blank" rel="noopener">GitHub</a>
            <a href="index.html">Home</a> 
          </div>
       <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
            ◐ Theme
          </button>
        </div>
        <div class="header-right">
          <div class="photo-frame">
              <img src="assets/img/samprit.jpeg" alt="Samprit Chakraborty" />
          </div>

        </div>
      </div>
    </header>

    <!-- Two-column layout -->
    <div class="page-layout">
      <!-- Left contents -->
      <aside class="side-menu">
        <div class="side-menu-title">Contents</div>
        <nav>
          <a href="index.html#projects">⟵ Back to Projects</a>
           <a href="#Introduction">Introduction</a>
          <a href="#Problem setup">Problem setup</a>
          <a href="#prior-evolution">Prior evolution</a>
          <a href="#Simulations">Posterior Mean vs True Coefficients</a>
          <a href="#Comparison with Lasso Relative to True Coefficients">Comparison with Lasso Relative to True Coefficients</a>
          <a href="#Comparison with OLS and LASSO">Comparison with OLS and LASSO</a>
          <a href="#convergence">Convergence diagnostics</a>
          <a href="#theorem">Simulation to Theory</a>
        </nav>
      </aside>

      <!-- Right content -->
      <main class="main-content">
        <!-- Overview -->
        <section id="Introduction" class="section">
          <h2>Introduction</h2>
          <p>
  On this page, I present the simulation results and theoretical findings from my working paper
  <em>Solving Empirical Bayes Estimation in High-Dimensional Linear Regression Using a Mean-Field Approach</em>,
  which is part of my Master's thesis supervised by
  <a href="https://sites.stat.columbia.edu/bodhi/Bodhi/Welcome.html" target="_blank" rel="noopener">Prof. Bodhisattva Sen</a>.
  This work focuses on the computational aspects of
  <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">
    A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression
  </a>.
  We develop an algorithm for this problem and compare its performance with LASSO, OLS, MCMC, and we also compare our algorithm with existing CAVI method.
  We also establish several theoretical results for our algorithm.
</p>
</section>

        <section id="Problem setup" class="section">
          <h2>Problem setup</h2>
          <p>
    We study an empirical Bayes approach to high-dimensional Gaussian linear regression. 
    The data follow
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \varepsilon,\quad 
    \varepsilon \sim \mathcal{N}\!\left(0, \sigma^2 \mathbf{I}_n\right),
    \]
  </p>

  <p>
    where \(\mathbf{y} \in \mathbb{R}^n\) is the response vector, 
    \(\mathbf{X} \in \mathbb{R}^{n \times p}\) is the design matrix with columns 
    \(\mathbf{x}_1, \dots, \mathbf{x}_p\), \(\boldsymbol{\beta} = (\beta_1,\dots,\beta_p)^\top\) 
    are the unknown regression coefficients, and \(\sigma^2 > 0\) is treated as known.
  </p>

  <p>
    In an empirical Bayes formulation, we place an unknown prior \(\mu\) on each coefficient and assume
    \(\beta_j \overset{\text{iid}}{\sim} \mu\) for \(j = 1,\dots,p\). 
    We approximate \(\mu\) by a discrete distribution supported on a fixed grid 
    \(\mathcal{A} = \{a_1 < \dots < a_k\} \subset \mathbb{R}\), for example a fine grid in \([-1.5, 1.5]\) (suppose support of the prior is \([-1, 1]\), so keep lower and upper value of these grid points a little relaxed):
  </p>

  <p style="text-align:center;">
    \[
    \mu(\mathrm{d}t) \approx \sum_{\ell=1}^k \pi_\ell \,\delta_{a_\ell}(\mathrm{d}t),
    \quad 
    \boldsymbol{\pi} = (\pi_1,\dots,\pi_k)^\top \in \Delta^{k-1},
    \]
  </p>

  <p>
    where \(\Delta^{k-1} = \{\boldsymbol{\pi}: \pi_\ell \ge 0,\ \sum_{\ell=1}^k \pi_\ell = 1\}\) is the simplex 
    and \(\delta_{a_\ell}\) is a point mass at \(a_\ell\). The empirical Bayes goal is to learn 
    \(\boldsymbol{\pi}\) from the data.
  </p>

  <p>
    Instead of maximizing the intractable marginal likelihood
    \(p(\mathbf{y} \mid \boldsymbol{\pi})\), we use a mean-field variational approximation, where we approximate the posterior with a factorized distribution from the mean-field family
  </p>

  <p style="text-align:center;">
    \[
    q(\boldsymbol{\beta}) = \prod_{j=1}^p q_j(\beta_j),
    \quad
    q_j(\beta_j) = \sum_{\ell=1}^k r_{j\ell}\,\delta_{a_\ell}(\beta_j),
    \quad 
    \mathbf{r}_j = (r_{j1}, \dots, r_{jk})^\top \in \Delta^{k-1},
    \]
  </p>

  <p>
    and jointly optimize the mixing weights \(\boldsymbol{\pi}\) and the local weights 
    \(\{\mathbf{r}_j\}_{j=1}^p\) by maximizing the corresponding evidence lower bound (ELBO).
    From the optimal variational distribution, we form the coordinate-wise posterior mean
  </p>
      <p style="text-align:center;">
    \[
    \boldsymbol{u}
    \;=\;
    \mathbb{E}_q[\boldsymbol{\beta}]
    =
    (u_1,\dots,u_p)^\top,
    \qquad
    u_j
    =
    \sum_{\ell=1}^k a_\ell\, r_{j\ell},
    \]
  </p>

  <p>
    which serves as the empirical Bayes estimator of \(\boldsymbol{\beta}\) produced by our algorithm.
    The simulation results below compare this estimator against OLS, Lasso, MCMC-based benchmarks,
    and existing CAVI-type methods.
  </p>
</section>
      

        <!-- Figures: stacked vertically -->
        <section id="Simulations" class="section">
  <h2>Simulation Setup</h2>

  <p>
    Unless stated otherwise, the simulation results below are based on the following baseline design.
  </p>

  <p style="text-align:center;">
    \[
    n = 800,\qquad p = 50.
    \]
  </p>

  <p>
    The design matrix \(\mathbf{X} \in \mathbb{R}^{n \times p}\) has entries
    \(\mathbf{X}_{ij} \sim \mathcal{N}(0, 1/n)\), i.e.
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{X} = \frac{1}{\sqrt{n}} \mathbf{Z},
    \quad \mathbf{Z}_{ij} \sim \mathcal{N}(0,1),
    \]
  </p>

  <p>
    so that \(\|\mathbf{x}_j\|^2 = O(1)\) for each column, in line with Assumption&nbsp;1 of
    <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">Mukherjee et al.</a>
  </p>

  <p>
    The true coefficients \(\boldsymbol{\beta}^\star\) are three-point mass:
  </p>

  <p style="text-align:center;">
    \[
    \beta_j^\star \in \{-1, 0, 1\},\quad
    \mathbb{P}(\beta_j^\star = -1) = 0.3,\ 
    \mathbb{P}(\beta_j^\star = 0) = 0.4,\ 
    \mathbb{P}(\beta_j^\star = 1) = 0.3,
    \]
  </p>

  <p>
    and the responses are generated as
  </p>

  <p style="text-align:center;">
    \[
    \mathbf{y} = \mathbf{X}\boldsymbol{\beta}^\star + \varepsilon,
    \qquad
    \varepsilon \sim \mathcal{N}(0, \sigma^2 \mathbf{I}_n),
    \]
  </p>

  <p>
    where \(\sigma^2\) is chosen to yield a non-trivial signal-to-noise regime; all methods are compared under this common design.
  </p>

</section>

      <section id="prior-evolution" class="section">
  <h2>Prior Evolution</h2>
  <p>
    The clip below shows how the estimated prior mass on the grid \(\mathcal{A}\) evolves over
    iterations of our algorithm. The mass gradually concentrates near the signal
    values, illustrating how the empirical Bayes procedure adapts to the underlying sparsity pattern.
  </p>

  <video class="veb-video" controls preload="metadata">
    <source src="assets/video/prior_evolution_strong_changes.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</section>
      <section id="Simulations" class="section">
        <h2>Posterior Mean vs True Coefficients</h2>
        <div class="project-card">
  <div class="project-name">Figure 1: Posterior Mean vs True Coefficients</div>
  <img
    src="assets/img/veb_posterior_mean_vs_true_beta.jpeg"
    alt="Scatter of posterior mean estimates versus true beta coefficients with bubble size proportional to posterior variance."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
    Posterior mean estimates from the VEB procedure track the true coefficient values closely;
    bubble size encodes the corresponding posterior variance.
  </div>
</div>
</section>
        <section id="Comparison with Lasso Relative to True Coefficients" class="section">
        <h2>Comparison with Lasso Relative to True Coefficients</h2>
        <div class="project-card">
  <div class="project-name">
    Figure 2: Comparison with Lasso
  </div>
  <img
    src="assets/img/veb_lasso_vs_veb_truebeta.jpeg"
    alt="Side-by-side comparison: Lasso estimates versus true coefficients (left) and variational posterior means versus true coefficients (right)."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
  Left: Lasso estimates versus true coefficients, showing shrinkage toward zero and noticeable dispersion around the diagonal.
  Right: VEB posterior means align closely with the diagonal at \(-1, 0, 1\), indicating better recovery of the true signal.
</div>
</div>
</section>
    <section id="Comparison with OLS and LASSO" class="section">
       <h2>Comparison with OLS and LASSO</h2>
        <div class="project-card">
  <div class="project-name">
    Figure 3: VEB Compared with OLS and Lasso
  </div>
  <img
    src="assets/img/veb_ols_lasso_comparison.jpeg"
    alt="Vertical line plots comparing VEB estimates to OLS (left) and Lasso (right), with true coefficients marked."
    style="width:100%; border-radius:10px; margin-top:4px;"
  />
  <div class="project-desc">
    Left: OLS versus VEB. Right: Lasso versus VEB. In both panels, vertical segments connect method-specific
    estimates to the same underlying coefficient, with red points marking the true values and orange points the
    VEB posterior means. The VEB estimates cluster tightly around the truth, while OLS and Lasso display larger
    dispersion and shrinkage effects.
  </div>
</div>
</section>
      <section id="convergence" class="section">
  <h2>Convergence Diagnostics</h2>
  <p>
    To illustrate stability of the variational procedure, we track the convergence of the estimated prior
    and the posterior mean across iterations of our algorithm.
  </p>

  <div class="video-row">
    <div class="video-col">
      <h3>W<sub>1</sub> Convergence of the Prior</h3>
      <p class="section-intro">
       Wasserstein-1 distance between the prior estimates at iterations \(t-1\) and \(t\).
        The decay toward zero shows stabilization of the learned prior on \(\mathcal{A}\).
      </p>
      <video class="veb-video" controls preload="metadata">
        <source src="assets/video/w1_convergence_700iter.mp4" type="video/mp4"/>
      </video>
    </div>
    <div class="video-col">
      <h3>L<sub>2</sub> Change in Posterior Mean</h3>
      <p class="section-intro">
        \(\big\|\hat{\boldsymbol{u}}^{(t)} - \hat{\boldsymbol{u}}^{(t-1)}\big\|_2\), the L<sub>2</sub>-norm of the
        difference between successive posterior mean iterates. Its decay toward zero reflects numerical
        convergence of the VEB posterior mean estimator.
      </p>
      <video class="veb-video" controls preload="metadata">
        <source src="assets/video/posterior_convergence_700iter.mp4" type="video/mp4" />
      </video>
    </div>
  </div>
</section>

      </main>
    </div>
  </div>
</section>
             <section id="theorem" class="section">
  <h2>Simulation to Theory</h2>

  <p style="text-align:center;">
    The two convergence videos above are not just a numerical coincidence: in this regime, the
    <span style="white-space:nowrap;">L<sub>2</sub> change in the posterior mean</span> and the
    <span style="white-space:nowrap;">W<sub>1</sub> change in the prior</span> really do decay at
    essentially the <em>same geometric rate</em>. The theorem below formalises exactly what the
    animations are hinting at.
  </p>

  <p style="text-align:center;">
    In these simulations the algorithm runs on a fixed 1D grid
    \(\{a_\ell\}_{\ell=1}^k \subset [-A,A]\) (here \(A = 1.5\)). At outer iteration
    \(t = 0,1,2,\dots\), we track:
  </p>

  <p class="p-bullet">the posterior means of the regression coefficients \(u^{(t)} \in \mathbb{R}^p\);</p>
  <p class="p-bullet">the learned prior weights \(\pi^{(t)} \in \Delta^{k-1}\) on the grid;</p>
  <p class="p-bullet">a two-layer update:</p>
  <p class="p-sub"><strong>EM step.</strong> Update \(\pi^{(t)} \mapsto \pi^{(t+1)}\) by averaging responsibilities across coordinates (standard EM for the mixing weights).</p>
  <p class="p-sub"><strong>Inner optimisation step (BFGS).</strong> With \(\pi^{(t+1)}\) fixed, refine the site-specific variational parameters via a short BFGS loop on the inner objective, starting from the previous iterate.</p>
  <p class="p-note">Unlike closed-form CAVI, this is a genuinely <em>two-layer EM + BFGS</em> scheme; the numerical inner solver matters for the convergence behaviour stated below.</p>

  <p style="text-align:center;">To measure convergence, we monitor:</p>

  <p style="text-align:center;">The L<sub>2</sub> change in the posterior mean:</p>
  <div class="math-block">
    \[
      \Delta_u^{(t)} := \big\| u^{(t)} - u^{(t-1)} \big\|_2 .
    \]
  </div>

  <p style="text-align:center;">The W<sub>1</sub> change in the prior:</p>
  <div class="math-block">
    \[
      \Delta_\pi^{(t)} := W_1\!\big(\pi^{(t)}, \pi^{(t-1)}\big),
    \]
  </div>
  <p>computed on the same fixed grid \(\{a_\ell\}\).</p>

  <p style="text-align:center;">The off-diagonal coupling induced by the design is captured by</p>
  <div class="math-block">
    \[
      M \;:=\; \frac{X^\top X}{\sigma^2}
      - \mathrm{diag}\!\Big(\frac{(X^\top X)_{11}}{\sigma^2},\dots,\frac{(X^\top X)_{pp}}{\sigma^2}\Big).
    \]
  </div>
    In our theorem we assume \(\|M\|_{\mathrm{op}} = O_{\mathbb{P}}(\sqrt{p/n})\); combined with the assumption
    \(p = o(n)\) in
    <a href="https://arxiv.org/pdf/2309.16843" target="_blank" rel="noopener">Mukherjee et al.</a>,
    this implies \(\|M\|_{\mathrm{op}} \to 0\). We state the theorem and omit the proof here.
  </p>

  <h4>Theorem (Common-ratio convergence for this EM + BFGS scheme)</h4>
  <p style="text-align:center;">
    The phenomenon observed in the videos is specific to the <em>two-layer</em> algorithm: an EM-style update of \(\pi\),
    followed by a local BFGS refinement of the site parameters. A closed-form batch CAVI step need not exhibit this.
  </p>

  <p>Assume:</p>
  <p class="p-bullet"><strong>Weak regression coupling:</strong> \(\|M\|_{\mathrm{op}} = O_{\mathbb{P}}(\sqrt{p/n})\), hence \(\|M\|_{\mathrm{op}} \to 0\) under \(p=o(n)\).</p>
  <p class="p-bullet"><strong>Non-degenerate limiting prior:</strong> The limit \(\pi^\star\) puts positive mass on at least two grid points.</p>
  <p class="p-bullet"><strong>Two-layer EM + BFGS update:</strong> Each outer iteration has (i) the EM update of \(\pi\), then (ii) a short BFGS run that attains first-order accuracy for the inner optimum.</p>

  <p>Then there exists \(\lambda \in (0,1)\) such that</p>
  <div class="math-block">
    \[
      \lim_{t\to\infty}\frac{\Delta_u^{(t-1)}}{\Delta_u^{(t)}} \;=\;
      \lim_{t\to\infty}\frac{\Delta_\pi^{(t-1)}}{\Delta_\pi^{(t)}} \;=\; \frac{1}{\lambda}.
    \]
  </div>

  <p>In particular, for large \(t\),</p>
  <div class="math-block">
    \[
      \Delta_u^{(t)} \approx c_u\,\lambda^{t}, \qquad
      \Delta_\pi^{(t)} \approx c_\pi\,\lambda^{t},
    \]
  </div>
  
  <p>
    <em>Remark:</em> The constant \(\lambda\) is the dominant eigenvalue of the local linearisation of the
    full EM + BFGS map around the fixed point \((u^\star,\pi^\star)\). In other words,
    \(\lambda\) is the effective contraction factor of one full outer iteration. Because the inner
    step is solved by BFGS to first order, the local behaviour of the algorithm is controlled by a
    single, well-defined \(\lambda\), and both diagnostics inherit this same rate.
    This is precisely why, in the videos above, the
    <span style="white-space:nowrap;">L<sub>2</sub> change in posterior mean</span>
    and the <span style="white-space:nowrap;">W<sub>1</sub> change in the prior</span>
    have almost identical “shape” on a log scale. The inner BFGS step makes the update behave locally like plugging in the exact site optimiser for the current prior.
    This “first-order exactness” drives the common-ratio limit; generic closed-form CAVI updates need not share it.
  </p>
  <p>
    <i>Degenerate edge case:</i> If the limiting prior \(\pi^\star\) collapses to a single grid point, both
    diagnostics drop to zero in finitely many steps and the ratio becomes vacuous; the theorem is
    intended for the more interesting regime where the limiting prior retains a genuinely
    high-dimensional support.
  </p>
</section>



  <footer class="footer">
    © <span id="year"></span> Samprit Chakraborty · VEB 
  </footer>

  <script src="assets/js/main.js"></script>
</body>
</html>
